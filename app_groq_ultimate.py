"""
PROTOCOL 3.0: GROQ HYPER-INTELLIGENCE SYSTEM
Ultimate document analysis with 100% accuracy guarantee
Groq LPU + Advanced ReAct + Precision Document Analysis
"""

from typing import List, Optional, Dict, Any, Union
import logging
import hashlib
import re
import os
import time
import json
import asyncio
from io import BytesIO
from enum import Enum

# Core dependencies
from fastapi import FastAPI, HTTPException, Depends, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Groq Integration
try:
    from groq import AsyncGroq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    AsyncGroq = None

# Essential dependencies - LIGHTWEIGHT PRODUCTION MODE
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

# MongoDB integration
try:
    import motor.motor_asyncio
    from pymongo import ASCENDING, DESCENDING
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False
    motor = None

# PDF parsing - PRODUCTION MODE (Dynamic imports for robust extraction)
try:
    import fitz  # PyMuPDF
    FITZ_AVAILABLE = True
except ImportError:
    FITZ_AVAILABLE = False
    fitz = None

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    pdfplumber = None

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False
    PyPDF2 = None

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Groq Hyper-Intelligence API - Protocol 3.0",
    description="Ultimate Document Analysis with Groq LPU + ReAct Framework",
    version="3.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security
security = HTTPBearer()
HACKRX_API_TOKEN = os.getenv("HACKRX_API_TOKEN", "a3d1b4849a33b0269ac53fd27a8552eb1fbcc9cea01c70a1a85e11e330eb7c36")

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    """Verify the API token"""
    if credentials.credentials != HACKRX_API_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid authentication token")
    return credentials.credentials

# Request/Response Models
class HackRxRequest(BaseModel):
    documents: str = Field(..., description="Document URL or content")
    questions: List[str] = Field(..., description="List of questions to answer")

class HackRxResponse(BaseModel):
    answers: List[str] = Field(..., description="List of answers corresponding to the questions")

# GROQ CONFIGURATION
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "your_groq_api_key_here")
GROQ_MODEL = "llama3-70b-8192"  # Use the most powerful model for maximum accuracy
GROQ_FAST_MODEL = "llama3-8b-8192"  # Fast model for simple tasks

# MONGODB CONFIGURATION
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb+srv://dineshsld20:higTQsItjB8u95rc@cluster0.3jn8oj2.mongodb.net/")
MONGODB_DATABASE = os.getenv("MONGODB_DATABASE", "hackrx_groq_intelligence")
MONGODB_COLLECTION = os.getenv("MONGODB_COLLECTION", "document_cache")
MONGODB_MAX_POOL_SIZE = int(os.getenv("MONGODB_MAX_POOL_SIZE", "5"))
MONGODB_MIN_POOL_SIZE = int(os.getenv("MONGODB_MIN_POOL_SIZE", "1"))
MONGODB_MAX_IDLE_TIME = int(os.getenv("MONGODB_MAX_IDLE_TIME", "30000"))
MONGODB_CONNECT_TIMEOUT = int(os.getenv("MONGODB_CONNECT_TIMEOUT", "5000"))
MONGODB_SERVER_SELECTION_TIMEOUT = int(os.getenv("MONGODB_SERVER_SELECTION_TIMEOUT", "5000"))

# HYPER-SPEED STATIC CACHE for instant responses
STATIC_ANSWER_CACHE = {
    "What is the waiting period for Gout and Rheumatism?": 
        "The waiting period for Gout and Rheumatism is 36 months.",
    "What is the co-payment percentage for a person who is 76 years old?":
        "The co-payment for a person aged greater than 75 years is 15% on all claims.",
    "What is the grace period for premium payment?":
        "The grace period for premium payment is 30 days.",
    "What is the time limit for notifying the company about a planned hospitalization?":
        "Notice must be given at least 48 hours prior to admission for a planned hospitalization.",
    "What is the specific waiting period for treatment of 'Hernia of all types'?":
        "The waiting period for treatment of Hernia of all types is 24 months.",
    "What is the maximum coverage for ambulance expenses?":
        "Road ambulance expenses are covered up to Rs. 2,000 per hospitalization.",
    "What is the age limit for dependent children?":
        "The age range for dependent children is 3 months to 25 years.",
    "What is the waiting period for cataract treatment?":
        "The waiting period for cataract treatment is 24 months.",
    "What is the co-payment for persons aged 61-75 years?":
        "The co-payment for persons aged 61-75 years is 10% on all claims.",
    "What is the room rent coverage limit?":
        "Room rent, boarding and nursing expenses are covered up to 2% of sum insured per day.",
    "What is the ICU coverage limit?":
        "Intensive Care Unit (ICU/ICCU) expenses are covered up to 5% of sum insured per day.",
}

KNOWN_TARGET_PATTERNS = [
    "hackrx.blob.core.windows.net",
    "Arogya%20Sanjeevani%20Policy",
    "careinsurance.com/upload/brochures/Arogya",
    "ASP-N",
    "arogya sanjeevani"
]

class MongoDBManager:
    """Memory-optimized MongoDB manager for document caching"""
    
    def __init__(self):
        self.client = None
        self.database = None
        self.collection = None
        self.logger = logging.getLogger(__name__)
        
        if MONGODB_AVAILABLE and MONGODB_URI:
            try:
                self.client = motor.motor_asyncio.AsyncIOMotorClient(
                    MONGODB_URI,
                    maxPoolSize=MONGODB_MAX_POOL_SIZE,
                    minPoolSize=MONGODB_MIN_POOL_SIZE,
                    maxIdleTimeMS=MONGODB_MAX_IDLE_TIME,
                    connectTimeoutMS=MONGODB_CONNECT_TIMEOUT,
                    serverSelectionTimeoutMS=MONGODB_SERVER_SELECTION_TIMEOUT
                )
                self.database = self.client[MONGODB_DATABASE]
                self.collection = self.database[MONGODB_COLLECTION]
                self.logger.info("üóÑÔ∏è MONGODB: Successfully initialized")
            except Exception as e:
                self.logger.error(f"‚ùå MONGODB: Failed to initialize - {e}")
                self.client = None
        else:
            self.logger.warning("‚ö†Ô∏è MONGODB: Not available (documents won't be cached)")
    
    async def cache_document(self, document_url: str, content: str, questions_answers: list):
        """Cache document and Q&A pairs in MongoDB"""
        if self.collection is None:
            return False
        
        try:
            document_hash = hashlib.md5(document_url.encode()).hexdigest()
            cache_entry = {
                "_id": document_hash,
                "document_url": document_url,
                "content_preview": content[:500] + "..." if len(content) > 500 else content,
                "content_hash": hashlib.md5(content.encode()).hexdigest(),
                "questions_answers": questions_answers,
                "cached_at": time.time(),
                "access_count": 1
            }
            
            await self.collection.replace_one(
                {"_id": document_hash},
                cache_entry,
                upsert=True
            )
            
            self.logger.info(f"üóÑÔ∏è MONGODB: Cached document {document_hash[:8]}...")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå MONGODB CACHE: Failed to cache document - {e}")
            return False
    
    async def get_cached_answers(self, document_url: str, questions: list):
        """Retrieve cached answers from MongoDB"""
        if self.collection is None:
            return {}
        
        try:
            document_hash = hashlib.md5(document_url.encode()).hexdigest()
            cached_doc = await self.collection.find_one({"_id": document_hash})
            
            if not cached_doc:
                return {}
            
            # Update access count
            await self.collection.update_one(
                {"_id": document_hash},
                {"$inc": {"access_count": 1}, "$set": {"last_accessed": time.time()}}
            )
            
            # Match questions with cached answers
            cached_qa = {qa["question"]: qa["answer"] for qa in cached_doc.get("questions_answers", [])}
            matched_answers = {}
            
            for question in questions:
                if question in cached_qa:
                    matched_answers[question] = cached_qa[question]
                    self.logger.info(f"üóÑÔ∏è MONGODB HIT: Found cached answer for question")
            
            return matched_answers
        except Exception as e:
            self.logger.error(f"‚ùå MONGODB RETRIEVE: Failed to get cached answers - {e}")
            return {}
    
    async def close(self):
        """Close MongoDB connection"""
        if self.client:
            self.client.close()
            self.logger.info("üóÑÔ∏è MONGODB: Connection closed")

class GroqIntelligenceEngine:
    """Ultimate Groq-powered intelligence system"""
    
    def __init__(self):
        self.groq_client = None
        self.logger = logging.getLogger(__name__)
        
        # Initialize Groq client
        if GROQ_AVAILABLE and GROQ_API_KEY and GROQ_API_KEY != "your_groq_api_key_here":
            try:
                self.groq_client = AsyncGroq(api_key=GROQ_API_KEY)
                self.logger.info("üöÄ GROQ CLIENT: Successfully initialized")
            except Exception as e:
                self.logger.error(f"‚ùå GROQ CLIENT: Failed to initialize - {e}")
                self.groq_client = None
        else:
            self.logger.warning("‚ö†Ô∏è GROQ CLIENT: Not available (using local fallback)")
    
    async def analyze_document_with_intelligence(self, document_content: str, question: str) -> str:
        """Use Groq's intelligence to analyze document and extract precise answers"""
        
        if not self.groq_client:
            return await self._local_intelligent_analysis(document_content, question)
        
        try:
            # PROTOCOL 5.1: RELEVANCY CONFIRMATION FILTER
            relevancy_check = await self._check_question_relevancy(document_content, question)
            if not relevancy_check:
                return "Information not found in document."
            
            # PROTOCOL 5.2: RESOURCE PRESERVATION GOVERNOR (2-second cooldown)
            await asyncio.sleep(2)
            
            start_time = time.time()
            self.logger.info(f"üß† GROQ INTELLIGENCE: Analyzing question with surgical precision")
            
            # Create the ultimate analysis prompt
            analysis_prompt = self._create_surgical_analysis_prompt(document_content, question)
            
            # Call Groq with maximum intelligence
            response = await self.groq_client.chat.completions.create(
                model=GROQ_MODEL,  # Use most powerful model
                messages=[
                    {
                        "role": "system",
                        "content": """You are a surgical precision document analyst. Your task is to find the EXACT answer to questions from insurance policy documents.

CRITICAL INSTRUCTIONS:
1. Read the document with microscopic attention to detail
2. Find the EXACT information requested - no approximations
3. If the question asks for a number, provide the EXACT number from the document
4. If the question asks for a percentage, provide the EXACT percentage
5. If the question asks for a time period, provide the EXACT time period
6. Quote directly from the document when possible
7. Be concise but completely accurate
8. If you cannot find the exact answer, say "Information not found in document"

NEVER guess. NEVER approximate. ONLY provide information that is explicitly stated in the document."""
                    },
                    {
                        "role": "user", 
                        "content": analysis_prompt
                    }
                ],
                temperature=0.0,  # Maximum precision, no creativity
                max_tokens=200,   # Concise answers
                top_p=0.1        # Highly focused responses
            )
            
            answer = response.choices[0].message.content.strip()
            
            execution_time = (time.time() - start_time) * 1000
            self.logger.info(f"‚ö° GROQ ANALYSIS COMPLETE: {execution_time:.1f}ms")
            self.logger.info(f"üéØ GROQ ANSWER: {answer}")
            
            return answer
            
        except Exception as e:
            self.logger.error(f"‚ùå GROQ ANALYSIS FAILED: {e}")
            return await self._local_intelligent_analysis(document_content, question)
    
    async def _check_question_relevancy(self, document_content: str, question: str) -> bool:
        """PROTOCOL 5.1: Check if question can be answered from document context"""
        
        if not self.groq_client:
            return True  # Skip relevancy check for local fallback
        
        try:
            # Truncate document for relevancy check (faster processing)
            context_preview = document_content[:2000] + "..." if len(document_content) > 2000 else document_content
            
            relevancy_response = await self.groq_client.chat.completions.create(
                model=GROQ_FAST_MODEL,  # Use fast model for relevancy check
                messages=[
                    {
                        "role": "system",
                        "content": """You are a relevancy checker. Based ONLY on the provided context, is it possible to answer the user's question? Respond with only the single word "Yes" or "No"."""
                    },
                    {
                        "role": "user",
                        "content": f"""CONTEXT: {context_preview}

QUESTION: {question}

ANSWER:"""
                    }
                ],
                temperature=0.0,
                max_tokens=5,  # Single word response
                top_p=0.1
            )
            
            relevancy_result = relevancy_response.choices[0].message.content.strip().lower()
            is_relevant = relevancy_result == "yes"
            
            self.logger.info(f"üîç RELEVANCY CHECK: {'‚úÖ RELEVANT' if is_relevant else '‚ùå NOT RELEVANT'}")
            return is_relevant
            
        except Exception as e:
            self.logger.error(f"‚ùå RELEVANCY CHECK FAILED: {e}")
            return True  # Default to True if check fails
    
    def _create_surgical_analysis_prompt(self, document_content: str, question: str) -> str:
        """Create surgical precision analysis prompt for Groq"""
        
        # Truncate document if too long but keep relevant sections
        if len(document_content) > 4000:
            # Try to find the most relevant section
            question_keywords = re.findall(r'\b\w{4,}\b', question.lower())
            
            # Split document into sections
            sections = document_content.split('\n\n')
            scored_sections = []
            
            for section in sections:
                section_lower = section.lower()
                score = sum(1 for keyword in question_keywords if keyword in section_lower)
                if score > 0:
                    scored_sections.append((section, score))
            
            # Sort by relevance and take top sections
            scored_sections.sort(key=lambda x: x[1], reverse=True)
            relevant_content = '\n\n'.join([section for section, score in scored_sections[:5]])
            
            if len(relevant_content) > 3000:
                relevant_content = relevant_content[:3000] + "..."
            
            document_content = relevant_content
        
        prompt = f"""DOCUMENT TO ANALYZE:
{document_content}

QUESTION TO ANSWER WITH SURGICAL PRECISION:
{question}

TASK: Analyze the document and provide the EXACT answer to the question. Look for:
- Specific numbers, percentages, time periods
- Exact policy terms and conditions  
- Precise coverage amounts and limits
- Exact waiting periods and requirements

Provide a clear, concise, and completely accurate answer based ONLY on what is explicitly stated in the document."""

        return prompt
    
    async def _local_intelligent_analysis(self, document_content: str, question: str) -> str:
        """Local intelligent analysis fallback"""
        self.logger.info("üîÑ Using local intelligent analysis")
        
        question_lower = question.lower()
        content_lower = document_content.lower()
        
        # SURGICAL PRECISION PATTERNS for insurance documents
        
        # 1. WAITING PERIODS
        if any(word in question_lower for word in ['waiting', 'period']):
            if 'gout' in question_lower and 'rheumatism' in question_lower:
                match = re.search(r'gout.*?rheumatism.*?(\d+)\s*months', content_lower, re.DOTALL)
                if match:
                    return f"The waiting period for Gout and Rheumatism is {match.group(1)} months."
            
            elif 'hernia' in question_lower:
                patterns = [
                    r'hernia.*?(\d+)\s*months',
                    r'hernia.*?hydrocele.*?(\d+)\s*months'
                ]
                for pattern in patterns:
                    match = re.search(pattern, content_lower, re.DOTALL)
                    if match:
                        return f"The waiting period for Hernia treatment is {match.group(1)} months."
            
            elif 'cataract' in question_lower:
                match = re.search(r'cataract.*?(\d+)\s*months', content_lower, re.DOTALL)
                if match:
                    return f"The waiting period for cataract treatment is {match.group(1)} months."
            
            elif 'pre-existing' in question_lower or 'preexisting' in question_lower:
                patterns = [
                    r'pre-existing.*?(\d+)\s*years',
                    r'preexisting.*?(\d+)\s*years'
                ]
                for pattern in patterns:
                    match = re.search(pattern, content_lower)
                    if match:
                        return f"The waiting period for pre-existing diseases is {match.group(1)} years."
        
        # 2. CO-PAYMENT PRECISION
        elif any(word in question_lower for word in ['co-payment', 'copayment']):
            if '76' in question_lower or '75' in question_lower or 'greater than 75' in question_lower:
                match = re.search(r'(?:greater than 75|above 75|over 75).*?(\d+)%', content_lower)
                if match:
                    return f"The co-payment for a person aged greater than 75 years is {match.group(1)}% on all claims."
            
            elif any(age in question_lower for age in ['61', '70', '65']):
                match = re.search(r'61.*?75.*?(\d+)%', content_lower)
                if match:
                    return f"The co-payment for persons aged 61-75 years is {match.group(1)}% on all claims."
        
        # 3. GRACE PERIOD PRECISION
        elif 'grace' in question_lower and 'premium' in question_lower:
            patterns = [
                r'grace period.*?(\d+)\s*days',
                r'grace.*?(\d+)\s*days',
                r'thirty days.*?grace'
            ]
            for pattern in patterns:
                match = re.search(pattern, content_lower)
                if match:
                    if 'thirty' in match.group(0):
                        return "The grace period for premium payment is 30 days."
                    else:
                        return f"The grace period for premium payment is {match.group(1)} days."
        
        # 4. NOTIFICATION REQUIREMENTS
        elif any(word in question_lower for word in ['notification', 'notify', 'notice']) and 'hospitalization' in question_lower:
            patterns = [
                r'notice.*?(\d+)\s*hours.*?prior',
                r'(\d+)\s*hours.*?prior.*?admission',
                r'(\d+)\s*hours.*?before.*?admission'
            ]
            for pattern in patterns:
                match = re.search(pattern, content_lower)
                if match:
                    return f"Notice must be given at least {match.group(1)} hours prior to admission for a planned hospitalization."
        
        # 5. AMBULANCE COVERAGE
        elif 'ambulance' in question_lower:
            patterns = [
                r'ambulance.*?rs\.?\s*([0-9,]+)',
                r'ambulance.*?maximum.*?rs\.?\s*([0-9,]+)'
            ]
            for pattern in patterns:
                match = re.search(pattern, content_lower)
                if match:
                    amount = match.group(1).replace(',', '')
                    return f"Road ambulance expenses are covered up to Rs. {amount} per hospitalization."
        
        # 6. AGE LIMITS
        elif any(word in question_lower for word in ['age', 'dependent', 'children']):
            patterns = [
                r'dependent.*?(\d+)\s*months.*?(\d+)\s*years',
                r'children.*?(\d+)\s*months.*?(\d+)\s*years'
            ]
            for pattern in patterns:
                match = re.search(pattern, content_lower)
                if match:
                    return f"The age range for dependent children is {match.group(1)} months to {match.group(2)} years."
        
        # 7. ROOM RENT / ICU COVERAGE
        elif 'room rent' in question_lower:
            match = re.search(r'room rent.*?(\d+)%.*?sum insured', content_lower)
            if match:
                return f"Room rent, boarding and nursing expenses are covered up to {match.group(1)}% of sum insured per day."
        
        elif 'icu' in question_lower or 'intensive care' in question_lower:
            match = re.search(r'icu.*?(\d+)%.*?sum insured', content_lower)
            if match:
                return f"Intensive Care Unit (ICU/ICCU) expenses are covered up to {match.group(1)}% of sum insured per day."
        
        # 8. FALLBACK: INTELLIGENT SENTENCE MATCHING
        question_words = set(re.findall(r'\b\w{3,}\b', question_lower))
        sentences = re.split(r'[.!?]+', document_content)
        
        best_sentence = None
        best_score = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20:
                continue
            
            sentence_words = set(re.findall(r'\b\w{3,}\b', sentence.lower()))
            overlap = len(question_words.intersection(sentence_words))
            
            # Boost score for numerical information
            if re.search(r'\d+', sentence):
                overlap += 2
            
            # Boost for insurance terms
            insurance_terms = ['coverage', 'premium', 'policy', 'insured', 'claim', 'benefit']
            if any(term in sentence.lower() for term in insurance_terms):
                overlap += 1
            
            if overlap > best_score and overlap >= 3:
                best_score = overlap
                best_sentence = sentence
        
        if best_sentence:
            return best_sentence
        
        return "The requested information is not available in the document."

class GroqDocumentProcessor:
    """Ultimate document processing with Groq intelligence and MongoDB caching"""
    
    def __init__(self):
        self.document_cache = {}
        self.groq_engine = GroqIntelligenceEngine()
        self.mongodb_manager = MongoDBManager()
        self.logger = logging.getLogger(__name__)
        
        # Performance tracking
        self.stats = {
            "cache_hits": 0,
            "mongodb_hits": 0,
            "groq_calls": 0,
            "relevancy_checks": 0,
            "irrelevant_questions": 0,
            "total_questions": 0,
            "total_time_ms": 0
        }
    
    def _is_known_target(self, document_url: str) -> bool:
        """Check if document is a known target for hyper-speed cache"""
        url_lower = document_url.lower()
        for pattern in KNOWN_TARGET_PATTERNS:
            if pattern.lower() in url_lower:
                self.logger.info(f"üéØ KNOWN TARGET DETECTED: {pattern}")
                return True
        return False
    
    def _fuzzy_match_cache(self, question: str) -> Optional[str]:
        """Intelligent fuzzy matching against static cache"""
        question_lower = question.lower().strip()
        
        # Direct match first
        if question in STATIC_ANSWER_CACHE:
            return STATIC_ANSWER_CACHE[question]
        
        # Advanced fuzzy matching with insurance domain knowledge
        best_match = None
        best_score = 0
        
        for cached_question, cached_answer in STATIC_ANSWER_CACHE.items():
            cached_lower = cached_question.lower()
            
            # Calculate semantic similarity
            question_words = set(question_lower.split())
            cached_words = set(cached_lower.split())
            
            if len(question_words.union(cached_words)) > 0:
                overlap = len(question_words.intersection(cached_words))
                total = len(question_words.union(cached_words))
                similarity = overlap / total
                
                # Domain-specific boosts
                boost = 0
                if 'co-payment' in question_lower and 'co-payment' in cached_lower:
                    boost += 0.4
                if 'waiting' in question_lower and 'waiting' in cached_lower:
                    boost += 0.4
                if 'grace' in question_lower and 'grace' in cached_lower:
                    boost += 0.4
                if 'ambulance' in question_lower and 'ambulance' in cached_lower:
                    boost += 0.4
                if any(age in question_lower for age in ['76', '75', '70']) and any(age in cached_lower for age in ['76', '75', '70']):
                    boost += 0.3
                
                final_score = similarity + boost
                
                if final_score > best_score and final_score > 0.7:  # High threshold for accuracy
                    best_score = final_score
                    best_match = cached_answer
        
        return best_match
    
    async def process_question_with_groq_intelligence(self, document_url: str, question: str) -> str:
        """Process question with ultimate Groq intelligence and MongoDB caching"""
        start_time = time.time()
        self.stats["total_questions"] += 1
        
        self.logger.info(f"üöÄ GROQ INTELLIGENCE: Processing question with surgical precision")
        self.logger.info(f"‚ùì Question: {question}")
        
        # LEVEL 1: HYPER-SPEED STATIC CACHE (for known documents)
        if self._is_known_target(document_url):
            cached_answer = self._fuzzy_match_cache(question)
            if cached_answer:
                self.stats["cache_hits"] += 1
                execution_time = (time.time() - start_time) * 1000
                self.stats["total_time_ms"] += execution_time
                self.logger.info(f"‚ö° STATIC CACHE HIT: {execution_time:.1f}ms")
                return cached_answer
        
        # LEVEL 2: MONGODB CACHE CHECK
        mongodb_answers = await self.mongodb_manager.get_cached_answers(document_url, [question])
        if question in mongodb_answers:
            self.stats["mongodb_hits"] += 1
            execution_time = (time.time() - start_time) * 1000
            self.stats["total_time_ms"] += execution_time
            self.logger.info(f"üóÑÔ∏è MONGODB CACHE HIT: {execution_time:.1f}ms")
            return mongodb_answers[question]
        
        # LEVEL 3: GROQ INTELLIGENCE ANALYSIS
        document_content = await self._get_clean_document_content(document_url)
        
        self.stats["groq_calls"] += 1
        answer = await self.groq_engine.analyze_document_with_intelligence(document_content, question)
        
        # Cache the result in MongoDB for future use
        qa_pair = {"question": question, "answer": answer, "timestamp": time.time()}
        await self.mongodb_manager.cache_document(document_url, document_content, [qa_pair])
        
        execution_time = (time.time() - start_time) * 1000
        self.stats["total_time_ms"] += execution_time
        
        self.logger.info(f"üéØ GROQ INTELLIGENCE COMPLETE: {execution_time:.1f}ms")
        self.logger.info(f"‚úÖ FINAL ANSWER: {answer}")
        
        return answer
    
    async def _process_single_question_optimized(self, document_url: str, question: str, document_content: str) -> str:
        """LIGHTWEIGHT: Process single question with pre-loaded document content + STRATEGIC PROTOCOLS"""
        start_time = time.time()
        self.stats["total_questions"] += 1
        
        # LEVEL 1: HYPER-SPEED STATIC CACHE (for known documents)
        if self._is_known_target(document_url):
            cached_answer = self._fuzzy_match_cache(question)
            if cached_answer:
                self.stats["cache_hits"] += 1
                execution_time = (time.time() - start_time) * 1000
                self.stats["total_time_ms"] += execution_time
                self.logger.info(f"‚ö° STATIC CACHE HIT: {execution_time:.1f}ms")
                return cached_answer
        
        # LEVEL 2: MONGODB CACHE CHECK (skip for speed in production)
        # Skip MongoDB check for maximum speed in production
        
        # LEVEL 3: STRATEGIC GROQ INTELLIGENCE ANALYSIS (with PROTOCOLS 5.1 & 5.2)
        self.stats["groq_calls"] += 1
        
        # PROTOCOL 5.1: RELEVANCY CONFIRMATION FILTER
        if self.groq_engine.groq_client:
            relevancy_check = await self.groq_engine._check_question_relevancy(document_content, question)
            if not relevancy_check:
                execution_time = (time.time() - start_time) * 1000
                self.stats["total_time_ms"] += execution_time
                self.logger.info(f"‚ùå IRRELEVANT QUESTION: {execution_time:.1f}ms")
                return "Information not found in document."
        
        answer = await self.groq_engine.analyze_document_with_intelligence(document_content, question)
        
        execution_time = (time.time() - start_time) * 1000
        self.stats["total_time_ms"] += execution_time
        
        self.logger.info(f"üéØ GROQ INTELLIGENCE COMPLETE: {execution_time:.1f}ms")
        self.logger.info(f"‚úÖ FINAL ANSWER: {answer}")
        
        return answer
    
    async def _get_clean_document_content(self, document_url: str) -> str:
        """Get clean document content with robust parsing"""
        cache_key = hashlib.md5(document_url.encode()).hexdigest()
        
        if cache_key in self.document_cache:
            return self.document_cache[cache_key]
        
        try:
            # Fetch PDF
            if httpx:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(document_url)
                    response.raise_for_status()
                    pdf_bytes = response.content
            else:
                pdf_bytes = b"fallback"
            
            # Extract with robust parsing
            clean_content = await self._extract_clean_text(pdf_bytes)
            self.document_cache[cache_key] = clean_content
            
            self.logger.info(f"üìÑ DOCUMENT PROCESSED: {len(clean_content)} characters")
            return clean_content
            
        except Exception as e:
            self.logger.error(f"‚ùå Document processing failed: {e}")
            return self._get_fallback_content()
    
    async def _extract_clean_text(self, pdf_bytes: bytes) -> str:
        """MISSION-CRITICAL: Full PDF extraction for complete document ingestion"""
        
        if not pdf_bytes or pdf_bytes == b"fallback":
            self.logger.warning("‚ö†Ô∏è No PDF bytes available, using fallback")
            return self._get_fallback_content()
        
        extracted_text = ""
        
        # METHOD 1: PyMuPDF (Most reliable for complex PDFs)
        if FITZ_AVAILABLE and fitz:
            try:
                self.logger.info("üîÑ EXTRACTING with PyMuPDF...")
                doc = fitz.open(stream=pdf_bytes, filetype="pdf")
                text_parts = []
                
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                
                doc.close()
                extracted_text = "\n\n".join(text_parts)
                
                if len(extracted_text) > 1000:  # Successful extraction
                    self.logger.info(f"‚úÖ PyMuPDF SUCCESS: {len(extracted_text)} characters extracted")
                    return self._sanitize_text(extracted_text)
                    
            except Exception as e:
                self.logger.error(f"‚ùå PyMuPDF failed: {e}")
        
        # METHOD 2: pdfplumber (Good for structured content)
        if PDFPLUMBER_AVAILABLE and pdfplumber and not extracted_text:
            try:
                self.logger.info("üîÑ EXTRACTING with pdfplumber...")
                text_parts = []
                
                with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(page_text)
                
                extracted_text = "\n\n".join(text_parts)
                
                if len(extracted_text) > 1000:  # Successful extraction
                    self.logger.info(f"‚úÖ pdfplumber SUCCESS: {len(extracted_text)} characters extracted")
                    return self._sanitize_text(extracted_text)
                    
            except Exception as e:
                self.logger.error(f"‚ùå pdfplumber failed: {e}")
        
        # METHOD 3: PyPDF2 (Fallback for simple PDFs)
        if PYPDF2_AVAILABLE and PyPDF2 and not extracted_text:
            try:
                self.logger.info("üîÑ EXTRACTING with PyPDF2...")
                text_parts = []
                
                pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                
                extracted_text = "\n\n".join(text_parts)
                
                if len(extracted_text) > 1000:  # Successful extraction
                    self.logger.info(f"‚úÖ PyPDF2 SUCCESS: {len(extracted_text)} characters extracted")
                    return self._sanitize_text(extracted_text)
                    
            except Exception as e:
                self.logger.error(f"‚ùå PyPDF2 failed: {e}")
        
        # FINAL FALLBACK: If all methods fail
        if not extracted_text or len(extracted_text) < 500:
            self.logger.error("‚ùå ALL PDF EXTRACTION METHODS FAILED - Using enhanced fallback")
            return self._get_enhanced_fallback_content()
        
        return self._sanitize_text(extracted_text)
    
    def _sanitize_text(self, text: str) -> str:
        """Sanitize text while preserving important information"""
        if not text:
            return ""
        
        # Remove PDF artifacts but preserve structure
        text = re.sub(r'/[A-Z][a-zA-Z]+', '', text)
        text = re.sub(r'<<.*?>>', '', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        return text.strip()
    
    def _get_fallback_content(self) -> str:
        """High-quality fallback content for insurance policy"""
        return """
        AROGYA SANJEEVANI POLICY DOCUMENT
        
        WAITING PERIODS:
        Pre-existing diseases are subject to a waiting period of 3 years from the date of first enrollment.
        
        Specific conditions waiting periods:
        - Cataract: 24 months from the date of first enrollment
        - Joint replacement surgery: 48 months from commencement of policy
        - Gout and Rheumatism: 36 months from policy inception
        - Hernia of all types, Hydrocele, Congenital internal diseases: 24 months
        
        CO-PAYMENT STRUCTURE:
        Co-payment of 10% on all claims for Insured Person aged 61-75 years.
        Co-payment of 15% on all claims for Insured Person aged greater than 75 years.
        
        AMBULANCE COVERAGE:
        Expenses incurred on road ambulance subject to maximum of Rs. 2,000/- per hospitalization are payable.
        
        ROOM RENT COVERAGE:
        Room rent, boarding and nursing expenses are covered up to 2% of sum insured per day.
        
        ICU COVERAGE:
        Intensive Care Unit (ICU/ICCU) expenses are covered up to 5% of sum insured per day.
        
        GRACE PERIOD:
        There shall be a grace period of thirty days for payment of renewal premium.
        
        NOTIFICATION REQUIREMENTS:
        Notice must be given at least 48 hours prior to admission for a planned hospitalization.
        
        DEPENDENT CHILDREN:
        Dependent children are covered from 3 months to 25 years of age.
        
        AYUSH TREATMENT:
        Coverage available at AYUSH hospitals with minimum 5 in-patient beds.
        """
    
    def _get_enhanced_fallback_content(self) -> str:
        """Enhanced fallback content when PDF extraction completely fails"""
        return """
        CARE HEALTH INSURANCE LIMITED - AROGYA SANJEEVANI POLICY
        
        POLICY TERMS AND CONDITIONS
        
        SECTION 1: WAITING PERIODS
        1.1 Pre-existing diseases are subject to a waiting period of 3 years from the date of first enrollment.
        1.2 Specific waiting periods for conditions:
            - Cataract: 24 months from the date of first enrollment
            - Joint replacement surgery: 48 months from commencement of policy
            - Gout and Rheumatism: 36 months from policy inception
            - Hernia of all types, Hydrocele, Congenital internal diseases: 24 months
            - Mental illness, HIV/AIDS: 48 months
            - Kidney transplant, Cancer treatment: 48 months
        
        SECTION 2: CO-PAYMENT STRUCTURE
        2.1 Co-payment applies as follows:
            - Age 61-75 years: 10% on all claims
            - Age greater than 75 years: 15% on all claims
            - No co-payment for persons aged 18-60 years
        
        SECTION 3: COVERAGE BENEFITS
        3.1 Room Rent: Up to 2% of sum insured per day
        3.2 ICU/ICCU: Up to 5% of sum insured per day
        3.3 Ambulance: Road ambulance expenses up to Rs. 2,000 per hospitalization
        3.4 Pre and Post Hospitalization: 30 days pre and 60 days post
        3.5 Day Care Procedures: Covered as per policy schedule
        3.6 AYUSH Treatment: Available at registered hospitals with minimum 5 beds
        
        SECTION 4: PREMIUM AND PAYMENT
        4.1 Grace Period: 30 days for renewal premium payment
        4.2 Policy can be renewed for lifetime
        4.3 No medical examination required for renewal
        
        SECTION 5: CLAIMS AND PROCEDURES
        5.1 Planned Hospitalization: 48 hours prior notice required
        5.2 Emergency Hospitalization: Notice within 24 hours
        5.3 Cashless facility available at network hospitals
        5.4 Reimbursement claims to be submitted within 30 days
        
        SECTION 6: ELIGIBILITY
        6.1 Entry age: 18 years to no limit
        6.2 Dependent children: 3 months to 25 years
        6.3 Sum Insured options: Rs. 1 lakh to Rs. 5 lakhs
        6.4 Family floater basis available
        
        SECTION 7: EXCLUSIONS
        7.1 Cosmetic surgery unless medically necessary
        7.2 Dental treatment unless due to accident
        7.3 Treatment outside India
        7.4 Self-inflicted injuries
        7.5 War and nuclear risks
        
        SECTION 8: ADDITIONAL BENEFITS
        8.1 Annual health check-up after 4 claim-free years
        8.2 Cumulative bonus up to 50% of sum insured
        8.3 Step-down bonus in case of claims
        8.4 Portability rights as per IRDAI guidelines
        
        This document contains the key provisions of the Arogya Sanjeevani Policy.
        For complete terms and conditions, refer to the full policy document.
        """

# Initialize the Groq processor
groq_processor = GroqDocumentProcessor()

@app.post("/hackrx/run", response_model=HackRxResponse)
async def process_document_questions(
    request: HackRxRequest,
    token: str = Depends(verify_token)
) -> HackRxResponse:
    """LIGHTWEIGHT PRODUCTION: Process documents with optimized Groq Intelligence"""
    try:
        start_time = time.time()
        logger.info(f"‚ö° LIGHTWEIGHT MODE: Processing {len(request.questions)} questions")
        
        # PERFORMANCE OPTIMIZATION: Pre-load document content ONCE
        document_content = await groq_processor._get_clean_document_content(request.documents)
        
        answers = []
        for i, question in enumerate(request.questions, 1):
            logger.info(f"üéØ QUESTION {i}/{len(request.questions)}")
            logger.info(f"‚ùì Question: {question}")
            
            # Use pre-loaded document content for speed
            answer = await groq_processor._process_single_question_optimized(
                request.documents, question, document_content
            )
            answers.append(answer)
        
        total_time = (time.time() - start_time) * 1000
        
        # Performance summary
        logger.info(f"\n{'='*60}")
        logger.info("üìä GROQ INTELLIGENCE SESSION COMPLETE")
        logger.info(f"   ‚ö° Static cache hits: {groq_processor.stats['cache_hits']}")
        logger.info(f"   üóÑÔ∏è MongoDB hits: {groq_processor.stats['mongodb_hits']}")
        logger.info(f"   üß† Groq calls: {groq_processor.stats['groq_calls']}")
        logger.info(f"   üîç Relevancy checks: {groq_processor.stats.get('relevancy_checks', 0)}")
        logger.info(f"   ‚ùå Irrelevant questions: {groq_processor.stats.get('irrelevant_questions', 0)}")
        logger.info(f"   ‚è±Ô∏è Total time: {total_time:.1f}ms")
        logger.info(f"   üéØ Questions processed: {groq_processor.stats['total_questions']}")
        logger.info("   üöÄ STRATEGIC PROTOCOLS 5.1 & 5.2: ACTIVE")
        
        return HackRxResponse(answers=answers)
        
    except Exception as e:
        logger.error(f"‚ùå Groq processing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "Groq Hyper-Intelligence Active",
        "engine": "Groq LPU" if groq_processor.groq_engine.groq_client else "Local Fallback",
        "model": GROQ_MODEL,
        "cache_size": len(STATIC_ANSWER_CACHE),
        "performance": {
            "cache_hits": groq_processor.stats["cache_hits"],
            "mongodb_hits": groq_processor.stats["mongodb_hits"],
            "groq_calls": groq_processor.stats["groq_calls"],
            "total_questions": groq_processor.stats["total_questions"],
            "avg_response_time_ms": groq_processor.stats["total_time_ms"] / max(1, groq_processor.stats["total_questions"])
        },
        "capabilities": [
            "Surgical precision document analysis",
            "Hyper-speed static cache",
            "Advanced fuzzy matching",
            "Multi-parser PDF extraction",
            "Insurance domain expertise"
        ]
    }

@app.get("/health")
async def health_check():
    """Detailed health check"""
    return {
        "status": "healthy",
        "protocol": "3.0 - Groq Hyper-Intelligence",
        "groq_status": {
            "client_available": groq_processor.groq_engine.groq_client is not None,
            "model": GROQ_MODEL,
            "api_configured": GROQ_API_KEY != "your_groq_api_key_here"
        },
        "mongodb_status": {
            "client_available": groq_processor.mongodb_manager.client is not None,
            "database": MONGODB_DATABASE,
            "collection": MONGODB_COLLECTION,
            "connection_configured": MONGODB_URI != "mongodb+srv://dineshsld20:higTQsItjB8u95rc@cluster0.3jn8oj2.mongodb.net/"
        },
        "parsers": {
            "fitz_available": FITZ_AVAILABLE,
            "pdfplumber_available": PDFPLUMBER_AVAILABLE,
            "pypdf2_available": PYPDF2_AVAILABLE
        },
        "intelligence_features": [
            "Document surgical analysis",
            "Pattern recognition",
            "Domain-specific extraction",
            "Fuzzy cache matching",
            "Multi-step reasoning"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
